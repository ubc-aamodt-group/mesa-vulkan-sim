.version 2.0
.target sm_10, map_f64_to_f32

// shader: MESA_SHADER_INTERSECTION
// inputs: 0
// outputs: 0
// uniforms: 0
// shared: 0
// decl_function main (0 params)
.entry MESA_SHADER_INTERSECTION_func4_main () {
	.reg .b64 %Sphere;
	rt_alloc_mem %Sphere, 64, 64; // decl_var ray_hit_attrib INTERP_MODE_NONE vec4 Sphere


	.reg .u64 %temp_u64;
	.reg .u32 %temp_u32;
	.reg .f32 %temp_f32;
	.reg .pred %temp_pred;
	.reg .f32 %const1_f32;
	mov.f32 %const1_f32, 0F3f800000;

	.reg .f32 %const0_f32;
	mov.f32 %const0_f32, 0F00000000;

	.reg .u32 %const0_u32;
	mov.u32 %const0_u32, 0;

	.reg .u16 %const1_u16;
	mov.u16 %const1_u16, 1;

	// start_block block_0:
	// preds: 
	.reg .f32 %ssa_0;
	mov.f32 %ssa_0, 0F00000000; // vec1 32 ssa_0 = load_const (0x00000000 /* 0.000000 */)
	.reg .b32 %ssa_0_bits;
	mov.b32 %ssa_0_bits, 0F00000000;

	.reg .u32 %ssa_1;
	load_ray_instance_custom_index %ssa_1;	// vec1 32 ssa_1 = intrinsic load_ray_instance_custom_index () ()

	.reg .b64 %ssa_2;
	load_vulkan_descriptor %ssa_2, 0, 9, 7; // vec2 32 ssa_2 = intrinsic vulkan_resource_index (%ssa_0) (0, 9, 7) /* desc_set=0 */ /* binding=9 */ /* desc_type=SSBO */

	.reg .b64 %ssa_3;
	mov.b64 %ssa_3, %ssa_2; // vec2 32 ssa_3 = intrinsic load_vulkan_descriptor (%ssa_2) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_4;
	mov.b64 %ssa_4, %ssa_3; // vec2 32 ssa_4 = deref_cast (SphereArray *)ssa_3 (ssbo SphereArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_5;
	add.u64 %ssa_5, %ssa_4, 0; // vec2 32 ssa_5 = deref_struct &ssa_4->Spheres (ssbo vec4[]) /* &((SphereArray *)ssa_3)->Spheres */

	.reg .b64 %ssa_6;
	.reg .u32 %ssa_6_array_index_32;
	.reg .u64 %ssa_6_array_index_64;
	mov.u32 %ssa_6_array_index_32, %ssa_1;
	mul.wide.u32 %ssa_6_array_index_64, %ssa_6_array_index_32, 16;
	add.u64 %ssa_6, %ssa_5, %ssa_6_array_index_64; // vec2 32 ssa_6 = deref_array &(*ssa_5)[ssa_1] (ssbo vec4) /* &((SphereArray *)ssa_3)->Spheres[ssa_1] */

	.reg .f32 %ssa_7_0;
	.reg .f32 %ssa_7_1;
	.reg .f32 %ssa_7_2;
	.reg .f32 %ssa_7_3;
	ld.global.f32 %ssa_7_0, [%ssa_6 + 0];
	ld.global.f32 %ssa_7_1, [%ssa_6 + 4];
	ld.global.f32 %ssa_7_2, [%ssa_6 + 8];
	ld.global.f32 %ssa_7_3, [%ssa_6 + 12];
// vec4 32 ssa_7 = intrinsic load_deref (%ssa_6) (16) /* access=16 */


	.reg .f32 %ssa_8_0;
	.reg .f32 %ssa_8_1;
	.reg .f32 %ssa_8_2;
	.reg .f32 %ssa_8_3;
	.reg .b64 %ssa_8_address;
	load_ray_world_origin %ssa_8_address; // vec3 32 ssa_8 = intrinsic load_ray_world_origin () ()
	ld.global.f32 %ssa_8_0, [%ssa_8_address + 0];
	ld.global.f32 %ssa_8_1, [%ssa_8_address + 4];
	ld.global.f32 %ssa_8_2, [%ssa_8_address + 8];
	ld.global.f32 %ssa_8_3, [%ssa_8_address + 12];

	.reg .f32 %ssa_9_0;
	.reg .f32 %ssa_9_1;
	.reg .f32 %ssa_9_2;
	.reg .f32 %ssa_9_3;
	.reg .b64 %ssa_9_address;
	load_ray_world_direction %ssa_9_address; // vec3 32 ssa_9 = intrinsic load_ray_world_direction () ()
	ld.global.f32 %ssa_9_0, [%ssa_9_address + 0];
	ld.global.f32 %ssa_9_1, [%ssa_9_address + 4];
	ld.global.f32 %ssa_9_2, [%ssa_9_address + 8];
	ld.global.f32 %ssa_9_3, [%ssa_9_address + 12];

	.reg .f32 %ssa_10;
	load_ray_t_min %ssa_10;	// vec1 32 ssa_10 = intrinsic load_ray_t_min () ()

	.reg .f32 %ssa_11;
	load_ray_t_max %ssa_11;	// vec1 32 ssa_11 = intrinsic load_ray_t_max () ()

	.reg .f32 %ssa_12;
	neg.f32 %ssa_12, %ssa_7_0; // vec1 32 ssa_12 = fneg ssa_7.x

	.reg .f32 %ssa_13;
	neg.f32 %ssa_13, %ssa_7_1; // vec1 32 ssa_13 = fneg ssa_7.y

	.reg .f32 %ssa_14;
	neg.f32 %ssa_14, %ssa_7_2; // vec1 32 ssa_14 = fneg ssa_7.z

	.reg .f32 %ssa_15;
	add.f32 %ssa_15, %ssa_8_0, %ssa_12; // vec1 32 ssa_15 = fadd ssa_8.x, ssa_12

	.reg .f32 %ssa_16;
	add.f32 %ssa_16, %ssa_8_1, %ssa_13; // vec1 32 ssa_16 = fadd ssa_8.y, ssa_13

	.reg .f32 %ssa_17;
	add.f32 %ssa_17, %ssa_8_2, %ssa_14; // vec1 32 ssa_17 = fadd ssa_8.z, ssa_14

	.reg .f32 %ssa_18;
	mul.f32 %ssa_18, %ssa_9_0, %ssa_9_0; // vec1 32 ssa_18 = fmul ssa_9.x, ssa_9.x

	.reg .f32 %ssa_19;
	mul.f32 %ssa_19, %ssa_9_1, %ssa_9_1; // vec1 32 ssa_19 = fmul ssa_9.y, ssa_9.y

	.reg .f32 %ssa_20;
	mul.f32 %ssa_20, %ssa_9_2, %ssa_9_2; // vec1 32 ssa_20 = fmul ssa_9.z, ssa_9.z

	.reg .f32 %ssa_21_0;
	.reg .f32 %ssa_21_1;
	.reg .f32 %ssa_21_2;
	.reg .f32 %ssa_21_3;
	mov.f32 %ssa_21_0, %ssa_18;
	mov.f32 %ssa_21_1, %ssa_19;
	mov.f32 %ssa_21_2, %ssa_20; // vec3 32 ssa_21 = vec3 ssa_18, ssa_19, ssa_20

	.reg .f32 %ssa_22;
	add.f32 %ssa_22, %ssa_21_0, %ssa_21_1;
	add.f32 %ssa_22, %ssa_22, %ssa_21_2; // vec1 32 ssa_22 = fsum3 ssa_21

	.reg .f32 %ssa_23;
	mul.f32 %ssa_23, %ssa_15, %ssa_9_0; // vec1 32 ssa_23 = fmul ssa_15, ssa_9.x

	.reg .f32 %ssa_24;
	mul.f32 %ssa_24, %ssa_16, %ssa_9_1; // vec1 32 ssa_24 = fmul ssa_16, ssa_9.y

	.reg .f32 %ssa_25;
	mul.f32 %ssa_25, %ssa_17, %ssa_9_2; // vec1 32 ssa_25 = fmul ssa_17, ssa_9.z

	.reg .f32 %ssa_26_0;
	.reg .f32 %ssa_26_1;
	.reg .f32 %ssa_26_2;
	.reg .f32 %ssa_26_3;
	mov.f32 %ssa_26_0, %ssa_23;
	mov.f32 %ssa_26_1, %ssa_24;
	mov.f32 %ssa_26_2, %ssa_25; // vec3 32 ssa_26 = vec3 ssa_23, ssa_24, ssa_25

	.reg .f32 %ssa_27;
	add.f32 %ssa_27, %ssa_26_0, %ssa_26_1;
	add.f32 %ssa_27, %ssa_27, %ssa_26_2; // vec1 32 ssa_27 = fsum3 ssa_26

	.reg .f32 %ssa_28;
	mul.f32 %ssa_28, %ssa_15, %ssa_15;	// vec1 32 ssa_28 = fmul ssa_15, ssa_15

	.reg .f32 %ssa_29;
	mul.f32 %ssa_29, %ssa_16, %ssa_16;	// vec1 32 ssa_29 = fmul ssa_16, ssa_16

	.reg .f32 %ssa_30;
	mul.f32 %ssa_30, %ssa_17, %ssa_17;	// vec1 32 ssa_30 = fmul ssa_17, ssa_17

	.reg .f32 %ssa_31_0;
	.reg .f32 %ssa_31_1;
	.reg .f32 %ssa_31_2;
	.reg .f32 %ssa_31_3;
	mov.f32 %ssa_31_0, %ssa_28;
	mov.f32 %ssa_31_1, %ssa_29;
	mov.f32 %ssa_31_2, %ssa_30; // vec3 32 ssa_31 = vec3 ssa_28, ssa_29, ssa_30

	.reg .f32 %ssa_32;
	add.f32 %ssa_32, %ssa_31_0, %ssa_31_1;
	add.f32 %ssa_32, %ssa_32, %ssa_31_2; // vec1 32 ssa_32 = fsum3 ssa_31

	.reg .f32 %ssa_33;
	mul.f32 %ssa_33, %ssa_7_3, %ssa_7_3; // vec1 32 ssa_33 = fmul ssa_7.w, ssa_7.w

	.reg .f32 %ssa_34;
	neg.f32 %ssa_34, %ssa_33;	// vec1 32 ssa_34 = fneg ssa_33

	.reg .f32 %ssa_35;
	add.f32 %ssa_35, %ssa_32, %ssa_34;	// vec1 32 ssa_35 = fadd ssa_32, ssa_34

	.reg .f32 %ssa_36;
	mul.f32 %ssa_36, %ssa_27, %ssa_27;	// vec1 32 ssa_36 = fmul ssa_27, ssa_27

	.reg .f32 %ssa_37;
	mul.f32 %ssa_37, %ssa_22, %ssa_35;	// vec1 32 ssa_37 = fmul ssa_22, ssa_35

	.reg .f32 %ssa_38;
	neg.f32 %ssa_38, %ssa_37;	// vec1 32 ssa_38 = fneg ssa_37

	.reg .f32 %ssa_39;
	add.f32 %ssa_39, %ssa_36, %ssa_38;	// vec1 32 ssa_39 = fadd ssa_36, ssa_38

	.reg .pred %ssa_40;
	setp.ge.f32 %ssa_40, %ssa_39, %ssa_0;	// vec1  1 ssa_40 = fge! ssa_39, ssa_0

	// succs: block_1 block_5 
	// end_block block_0:
	//if
	@!%ssa_40 bra else_25;
	
		// start_block block_1:
		// preds: block_0 
		.reg .f32 %ssa_41;
		neg.f32 %ssa_41, %ssa_27;	// vec1 32 ssa_41 = fneg ssa_27

		.reg .f32 %ssa_42;
		sqrt.approx.f32 %ssa_42, %ssa_39;	// vec1 32 ssa_42 = fsqrt ssa_39

		.reg .f32 %ssa_43;
		neg.f32 %ssa_43, %ssa_42;	// vec1 32 ssa_43 = fneg ssa_42

		.reg .f32 %ssa_44;
		add.f32 %ssa_44, %ssa_41, %ssa_43;	// vec1 32 ssa_44 = fadd ssa_41, ssa_43

		.reg .f32 %ssa_45;
		rcp.approx.f32 %ssa_45, %ssa_22;	// vec1 32 ssa_45 = frcp ssa_22

		.reg .f32 %ssa_46;
		mul.f32 %ssa_46, %ssa_44, %ssa_45;	// vec1 32 ssa_46 = fmul ssa_44, ssa_45

		.reg .f32 %ssa_47;
		add.f32 %ssa_47, %ssa_41, %ssa_42;	// vec1 32 ssa_47 = fadd ssa_41, ssa_42

		.reg .f32 %ssa_48;
		mul.f32 %ssa_48, %ssa_47, %ssa_45;	// vec1 32 ssa_48 = fmul ssa_47, ssa_45

		.reg .pred %ssa_49;
		setp.ge.f32 %ssa_49, %ssa_46, %ssa_10;	// vec1  1 ssa_49 = fge! ssa_46, ssa_10

		.reg .pred %ssa_50;
		setp.lt.f32 %ssa_50, %ssa_46, %ssa_11;	// vec1  1 ssa_50 = flt! ssa_46, ssa_11

		.reg .pred %ssa_51;
		and.pred %ssa_51, %ssa_49, %ssa_50;	// vec1  1 ssa_51 = iand ssa_49, ssa_50

		.reg .pred %ssa_52;
		setp.ge.f32 %ssa_52, %ssa_48, %ssa_10;	// vec1  1 ssa_52 = fge! ssa_48, ssa_10

		.reg .pred %ssa_53;
		setp.lt.f32 %ssa_53, %ssa_48, %ssa_11;	// vec1  1 ssa_53 = flt! ssa_48, ssa_11

		.reg .pred %ssa_54;
		and.pred %ssa_54, %ssa_52, %ssa_53;	// vec1  1 ssa_54 = iand ssa_52, ssa_53

		.reg .pred %ssa_55;
		or.pred %ssa_55, %ssa_51, %ssa_54;	// vec1  1 ssa_55 = ior ssa_51, ssa_54

		// succs: block_2 block_3 
		// end_block block_1:
		//if
		@!%ssa_55 bra else_26;
		
			// start_block block_2:
			// preds: block_1 
			.reg .b64 %ssa_56;
	mov.b64 %ssa_56, %Sphere; // vec1 32 ssa_56 = deref_var &Sphere (ray_hit_attrib vec4) 

	st.global.f32 [%ssa_56 + 0], %ssa_7_0;
	st.global.f32 [%ssa_56 + 4], %ssa_7_1;
	st.global.f32 [%ssa_56 + 8], %ssa_7_2;
	st.global.f32 [%ssa_56 + 12], %ssa_7_3;
// intrinsic store_deref (%ssa_56, %ssa_7) (15, 0) /* wrmask=xyzw */ /* access=0 */


			.reg  .f32 %ssa_57;
			selp.f32 %ssa_57, %ssa_46, %ssa_48, %ssa_51; // vec1 32 ssa_57 = bcsel ssa_51, ssa_46, ssa_48

			.reg .pred %ssa_58;
			report_ray_intersection.pred %ssa_58, %ssa_57, %ssa_0;	// vec1  1 ssa_58 = intrinsic report_ray_intersection (%ssa_57, %ssa_0) ()

			// succs: block_4 
			// end_block block_2:
			bra end_if_26;
		
		else_26: 
			// start_block block_3:
			// preds: block_1 
			// succs: block_4 
			// end_block block_3:
		end_if_26:
		// start_block block_4:
		// preds: block_2 block_3 
		// succs: block_6 
		// end_block block_4:
		bra end_if_25;
	
	else_25: 
		// start_block block_5:
		// preds: block_0 
		// succs: block_6 
		// end_block block_5:
	end_if_25:
	// start_block block_6:
	// preds: block_4 block_5 
	// succs: block_7 
	// end_block block_6:
	// block block_7:
	shader_exit:
	ret ;
}
